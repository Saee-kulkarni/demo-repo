import numpy as nm 
import matplotlib.pyplot as mtp 
import pandas as pd 
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC # "Support vector classifier" 
from sklearn.metrics import confusion_matrix
from sklearn.preprocessing import scale
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.metrics import confusion_matrix 
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import zero_one_loss
from sklearn.metrics import recall_score,precision _score
from sklearn.clustor import Kmeans,k_means
from sklearn.decomposition import PCA

The random state hyperparameter in the train_test_split() function controls the shuffling process.
//////////////////////////////////////////////////////////////////////////////
gradient discent

current_x=2
rate=0.01
precision=0.000001
previous_step_size=1
max_iters=10
iters=0
df=lambda x: 2*(x+3)

while previous_step_size > precision and iters < max_iters:
  previous_x = current_x
  current_x = current_x - rate*df(previous_x)
  previous_step_size=abs(current_x - previous_x)
  iters = iters+1
  print("Iterations", iters,"\nX value is", current_x)


///////////////////////////////////////////////////////////////////////////
ass1
data.tail()
data1 = data.drop(['Unnamed: 0','pickup_datetime','key'], axis=1)
correlation=data1.corr()
sns.heatmap(correlation, annot=True)
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
x = data1.drop('fare_amount', axis=1)
y = data1['fare_amount']
xtrain, xtest, ytrain, ytest = train_test_split(x,y,test_size=0.33, random_state=75)
scalar = StandardScaler()
xtrain = scalar.fit_transform(xtrain)
xtest = scalar.fit_transform(xtest)
from sklearn.linear_model import LinearRegression
linearmodel = LinearRegression()
linearmodel.fit(xtrain, ytrain)
prediction = linearmodel.predict(xtest)
from sklearn.metrics import r2_score, mean_squared_error
print(r2_score(ytest, prediction))
print(mean_squared_error(ytest, prediction))
from sklearn.ensemble import RandomForestRegressor
randomforestregressor.fit(xtrain,ytrain)
print("r2_score:",r2_score(ytest, randomforestprediction))
print("RMSE:",mean_squared_error(ytest, randomforestprediction))



/////////////////////////////////////////////////////////////////////////////////////////////
assi 2


x = data_set.drop(['Prediction'],axis=1)
y = data_set['Prediction']
from sklearn.preprocessing import scale
x = scale(x)
x_train, x_test, y_train, y_test= train_test_split(x, y, test_size= 0.3, random_sta
#Fitting K-NN classifier to the training set 
from sklearn.neighbors import KNeighborsClassifier 
classifier= KNeighborsClassifier(n_neighbors=7) 
classifier.fit(x_train, y_train) 
#Predicting the test set result 
y_pred= classifier.predict(x_test) 
print('prediction',y_pred)
#Creating the Confusion matrix 
from sklearn.metrics import confusion_matrix 
cm= confusion_matrix(y_test, y_pred) 
print('accuracy by KNN', cm)]
#SVM Classifier
from sklearn.svm import SVC # "Support vector classifier" 
#cost c=1
model = SVC(C=1)
#fit
model.fit(x_train, y_train)
y_pred = model.predict(x_test)
#Creating the Confusion matrix 
from sklearn.metrics import confusion_matrix 
cm2= confusion_matrix(y_test, y_pred) 
print('SVM', cm2)

/////////////////////////////////////////////////////////////////////////
assi 4(diabites)

df_problem_rows = df[(df['BloodPressure']==0) | (df['SkinThickness']==0) | (df['BMI'
data = pd.concat([df,df_problem_rows]).drop_duplicates(keep=False)
X = data.drop(["Outcome"] , axis =1)
y = data["Outcome"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaler.fit(X_train)
scaler.transform(X_train)
scaler.transform(X_test)
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
# Calculate the accuracy of the model
print(round(knn.score(X_test, y_test)*100 , 3) , "%")
from sklearn.metrics import zero_one_loss
error_rate = zero_one_loss(y_test, y_pred)
error_rate
from sklearn.metrics import recall_score
print("Recall : " , (recall_score(y_test, y_pred, average='binary'))*100 )


//////////////////////////////////////////////////////////////////////////////////

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
df = pd.read_csv("sales_data_sample.csv",encoding='latin1'
df = df[['QUANTITYORDERED', 'ORDERLINENUMBER']]
df = df.dropna(axis = 0)
wcss = []
for i in range(1, 11):
 clustering = KMeans(n_clusters=i, init='k-means++', random_state=42)
 clustering.fit(df)
 wcss.append(clustering.inertia_)
 
ks = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
sns.lineplot(x = ks, y = wcss);
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,5))
sns.scatterplot(ax=axes[0], data=df, x='QUANTITYORDERED', y='ORDERLINENUMBER').set_t
sns.scatterplot(ax=axes[1], data=df, x='QUANTITYORDERED', y='ORDERLINENUMBER', hue=

////////////////////////////////////////////////////////////////////////////////////